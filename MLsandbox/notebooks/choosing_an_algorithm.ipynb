{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an algorithm when you have a...\n",
    "\n",
    "#### Binary target variable\n",
    "\n",
    "Try logistic regression, probit\n",
    "\n",
    "#### Categorical target variable\n",
    "\n",
    "- Try multinomial logistic regression in R\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis and Quadratic Discriminant Analysis (LDA and QDA)\n",
    "- Decision Trees or Random Forests\n",
    "- k-Nearest Neighbors\n",
    "\n",
    "#### Ordinal target variable\n",
    "\n",
    "Try an ordinal logistic regression. Mord offers a newer [python implementation](http://pythonhosted.org/mord/), but it may be wise to do this using a [Proportional Odds Logistic Regression in R](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/polr.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are many ways to characterize algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Generative versus Discriminative Algorithms\n",
    "\n",
    "#### Generative models\n",
    "Attempt to model the conditional probability and prior distribution functions of all features in order to understand the most likely outcome, given a set of features. \n",
    "\n",
    "$$ P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$$\n",
    "\n",
    "So in order to find P(y|X), we first have to estimate the likelihood P(X|y) and the prior distribution P(y). [(see Appendix I for more info)](#why-no-px).  This requires a maximum likelihood (or minimum error) estimation, also known as the MLE. \n",
    "\n",
    "**Bonus 1: ** If you create a generative model, you can use the prior distributions to *generate* synthetic data\n",
    "\n",
    "**Bonus 2: ** Generative models typically outperform Discriminative models for smaller datasets, as they are less prone to overfitting\n",
    "\n",
    "#### Examples of generative models\n",
    "- Naive Bayes\n",
    "- LDA and QDA\n",
    "\n",
    "#### Discriminative models\n",
    "Attempt to find a discriminative boundary between classes by directly estimating posterior possibilities.\n",
    "$$ P(y|X) $$\n",
    "\n",
    "**Bonus 1: ** Tend to outperform generative models with larger datasets, as they learn P(y|X) directly\n",
    "\n",
    "**Bonus 2: ** Discriminative models do not require as many assumptions about the joint distribution structure of your features, such as their conditional independence.\n",
    "\n",
    "#### Examples of discriminative models\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- kNN\n",
    "- neural networks\n",
    "\n",
    "#### More details:\n",
    "1. [Stats.StackExchange](http://stats.stackexchange.com/questions/12421/generative-vs-discriminative)\n",
    "2. [A useful lecture from Columbia](https://www.ee.columbia.edu/~dpwe/e6820/lectures/L03-ml.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Parametric versus non-parametric\n",
    "\n",
    "### Parametric algorithms\n",
    "Start with a particular mapping function form, such as this linear combination:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1\\chi_1 + \\beta_2\\chi_2 + \\beta_3\\chi_1\\chi_2$$\n",
    "\n",
    "Features may be transformed to behave according to the form's underlying assumptions (e.g. normality or nonlinearity), and the coefficients are found by fitting the training data to the model's form.  \n",
    "\n",
    "In fact, many parametric models involve transformations on the exponential family to make the model operate as a linear combination of features.  Examples, such as the logistic regression, can be seen here: http://www.cs.princeton.edu/courses/archive/spr09/cos513/scribe/lecture11.pdf  \n",
    "\n",
    "#### Examples: #### \n",
    "- Logistic Regression\n",
    "- Linear Discriminant Analysis\n",
    "- Perceptron\n",
    "- Naive Bayes\n",
    "- Simple Neural Networks\n",
    "\n",
    "#### Choose a parametric algorithm when: ####\n",
    "- you want the results to be interpretable and insightful, not just predictive\n",
    "- you know how the distributions should behave, and you may not have a very large dataset\n",
    "- you have intuition into how your features behave and interact to produce a target variable\n",
    "\n",
    "#### Beware of: ####\n",
    "- over-generalization and lack of flexibility \n",
    "\n",
    "### Non-parametric algorithms\n",
    "\n",
    "Are more flexible ways to fit to the underlying data while maintaining the ability to generalize to new datasets\n",
    "\n",
    "#### Examples: ####\n",
    "- k-Nearest Neighbors\n",
    "- Decision Trees\n",
    "- Support Vector Machines\n",
    "\n",
    "#### Choose a non-parametric algorithm when: ####\n",
    "- you don't have an ideal mapping function that applies to every case\n",
    "- you have a lot of data\n",
    "\n",
    "#### Beware of: ####\n",
    "- overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "\n",
    "<a name=\"why-no-px\"> I. Modeling the joint likelihood for generative models</a> \n",
    "\n",
    "We are trying to find P(y|X) given the following function: $$ P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$$\n",
    "\n",
    "In this case, the normalization function P(X) does not need to be estimated in order to find $\\underset{y}{\\operatorname{<argmax>}} P(X|y)P(y),$ as it is invariant with respect to y. Furthermore, $P(X|y)P(y) = P(X,y),$\n",
    "so what generative models are finding is the joint likelihood P(X,y) instead of modeling the conditional likelihood P(y|X) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
