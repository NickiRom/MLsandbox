{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an algorithm when you have a...\n",
    "\n",
    "(note: this is a starting list and by no means exhaustive!)\n",
    "\n",
    "#### Binomial target variable\n",
    "\n",
    "- logistic regression, probit\n",
    "- random forest\n",
    "- SVM\n",
    "- kNN\n",
    "\n",
    "#### Categorical target variable\n",
    "\n",
    "- Try multinomial logistic regression in R\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis and Quadratic Discriminant Analysis (LDA and QDA)\n",
    "- Decision Trees or Random Forests\n",
    "- k-Nearest Neighbors\n",
    "\n",
    "#### Ordinal target variable\n",
    "\n",
    "Try an ordinal logistic regression. Mord offers a newer [python implementation](http://pythonhosted.org/mord/), but it may be wise to do this using a [Proportional Odds Logistic Regression in R](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/polr.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are many ways to characterize algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Generative versus Discriminative Algorithms\n",
    "\n",
    "#### Generative models\n",
    "Attempt to model the conditional probability and prior distribution functions of all features in order to understand the most likely outcome, given a set of features. \n",
    "\n",
    "$$ P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$$\n",
    "\n",
    "So in order to find P(y|X), we first have to estimate the likelihood P(X|y) and the prior distribution P(y). [(see Appendix I for more info)](#why-no-px).  This requires a maximum likelihood (or minimum error) estimation, also known as the MLE. \n",
    "\n",
    "**Bonus 1: ** If you create a generative model, you can use the prior distributions to *generate* synthetic data\n",
    "\n",
    "**Bonus 2: ** Generative models typically outperform Discriminative models for smaller datasets, as they are less prone to overfitting\n",
    "\n",
    "#### Examples of generative models\n",
    "- Naive Bayes\n",
    "- LDA and QDA\n",
    "\n",
    "#### Discriminative models\n",
    "Attempt to find a discriminative boundary between classes by directly estimating posterior possibilities.\n",
    "$$ P(y|X) $$\n",
    "\n",
    "**Bonus 1: ** Tend to outperform generative models with larger datasets, as they learn P(y|X) directly\n",
    "\n",
    "**Bonus 2: ** Discriminative models do not require as many assumptions about the joint distribution structure of your features, such as their conditional independence.\n",
    "\n",
    "#### Examples of discriminative models\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- kNN\n",
    "- neural networks\n",
    "\n",
    "#### More details:\n",
    "1. [Stats.StackExchange](http://stats.stackexchange.com/questions/12421/generative-vs-discriminative)\n",
    "2. [A useful lecture on generative and discriminative models from Columbia](https://www.ee.columbia.edu/~dpwe/e6820/lectures/L03-ml.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Parametric versus non-parametric\n",
    "\n",
    "### Parametric algorithms\n",
    "Start with a particular mapping function form, such as this linear combination:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1\\chi_1 + \\beta_2\\chi_2 + \\beta_3\\chi_1\\chi_2 + ...$$\n",
    "\n",
    "Features may be transformed to behave according to the form's underlying assumptions (e.g. normality of residual errors), and the coefficients are found by fitting the training data to the above form. The structural assumptions are generally validated by observing the distribution of residual errors using a [Q-Q Plot](http://data.library.virginia.edu/understanding-q-q-plots/).  \n",
    "\n",
    "In the case of a linear regression (aka **General Linear Model**), the Q-Q plot is testing for a normality assumption.  But the concept of fitting to a linear combination of features can be extended to many other types of data distributions as well.  In fact, the type of data (e.g. continuous, categorical, mixed) often influences the distribution of the residual errors.  In these cases, referred to as **Generalized Linear Models**, the mapping function is wrangled into a form that still looks like a linear combination of features, but which has a **link** function that can explain the connection between the features and the distribution of residual errors.  Here are some link functions for generalized linear models that allow the features to be expressed as a linear combination:\n",
    "\n",
    "\n",
    "[**Linear regression: **](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- takes continuous features\n",
    "- predicts a continuous outcome (the expected value of y, denoted as \"E(y)\")\n",
    "- therefore, the appropriate link function is: an identity link\n",
    "\n",
    "$$ E(y) = \\beta_0 + \\beta_1\\chi_1 + \\beta_2\\chi_2 + \\beta_3\\chi_1\\chi_2 + ...$$\n",
    "\n",
    "[**Logistic regression: **](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- takes mixed categorical, binary, and continuous features\n",
    "- predicts a binomial (yes or no) outcome\n",
    "- therefore, the appropriate link function is: a logit link\n",
    "\n",
    "$$ log\\left( \\frac{p}{1-p} \\right) = \\beta_0 + \\beta_1\\chi_1 + \\beta_2\\chi_2 + \\beta_3\\chi_1\\chi_2 + ...$$\n",
    "\n",
    "\n",
    "[**Poisson regression: **](http://scikit-learn.org/stable/modules/linear_model.html)\n",
    "- takes mixed categorical, binary, and continuous features\n",
    "- predicts a count (number) of events, whose residuals are Poisson-distributed (as event counts tend to be)\n",
    "- example: based on these campaign criteria, how many conversions do I expect to achieve?\n",
    "- therefore, the appropriate link function is: a log link\n",
    "\n",
    "$$ log\\left(\\mu\\right) = \\beta_0 + \\beta_1\\chi_1 + \\beta_2\\chi_2 + \\beta_3\\chi_1\\chi_2 + ...$$\n",
    "\n",
    "[More examples of canonical link functions](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch13.pdf)\n",
    "\n",
    "#### Examples of parametric algorithms: #### \n",
    "- Linear regression\n",
    "- Logistic Regression\n",
    "- Linear Discriminant Analysis\n",
    "- Perceptron\n",
    "- Naive Bayes\n",
    "- Simple neural networks (in general, these are somewhere between parametric and non-parametric)\n",
    "\n",
    "#### Choose a parametric algorithm when: ####\n",
    "- you want the results to be interpretable and insightful, not just predictive\n",
    "- you know how the distributions should behave, and you may not have a very large dataset\n",
    "- you have intuition into how your features behave and interact to produce a target variable\n",
    "\n",
    "#### Beware of: ####\n",
    "- over-generalization and lack of flexibility \n",
    "\n",
    "#### Summary: ####\n",
    "\n",
    "A linear model, also called a general linear model is a linear combination of features with a gaussian distribution of residual errors.  A *generalized* linear model is a linear combination of features with a predictable distribution of residual errors that are defined by an appropriate link function. So far, everything is parametric.\n",
    "\n",
    "**What happens when you model a linear combination of *functions* instead of features?**\n",
    "This is where **Generalized Linear Models** turn into their non-parametric counterparts, [**Generalized Additive Models**](https://en.wikipedia.org/wiki/Generalized_additive_model).  The form of a GAM is:\n",
    "\n",
    "$$ link(E(y)) = \\beta_0 + f_1(\\chi_1) + f_2(\\chi_2) + f_3(\\chi_1,\\chi_2) + ...$$\n",
    "\n",
    "where every function $f_n$ can be a parametric or non-parametric model in itself.  For more information, refer to [Tibshirani's original publication](http://web.stanford.edu/~hastie/Papers/gam.pdf) or [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\n",
    "\n",
    "### Non-parametric algorithms\n",
    "\n",
    "Are more flexible ways to fit to the underlying data while maintaining the ability to generalize to new datasets. So far, you've seen that combining parametric generalized linear models into a single additive model is one way to generate a non-parametric algorithms.  Alternatively, decision trees and support vector machines build non-parametric models by carving the feature space into sub-areas that are not easily described by probability distribution functions.\n",
    "\n",
    "#### Examples of non-parametric algorithms: ####\n",
    "- k-Nearest Neighbors\n",
    "- Decision Trees\n",
    "- Support Vector Machines\n",
    "\n",
    "#### Choose a non-parametric algorithm when: ####\n",
    "- you don't have an ideal mapping function that applies to every case\n",
    "- you have a lot of data\n",
    "\n",
    "#### Beware of: ####\n",
    "- overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "\n",
    "<a name=\"why-no-px\"> I. Modeling the joint likelihood for generative models</a> \n",
    "\n",
    "We are trying to find P(y|X) given the following function: $$ P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$$\n",
    "\n",
    "In this case, the normalization function P(X) does not need to be estimated in order to find $\\underset{y}{\\operatorname{<argmax>}} P(X|y)P(y),$ as it is invariant with respect to y. Furthermore, $P(X|y)P(y) = P(X,y),$\n",
    "so what generative models are finding is the joint likelihood P(X,y) instead of modeling the conditional likelihood P(y|X) directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
